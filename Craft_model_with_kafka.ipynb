{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4217afd-510c-43d9-b7a7-43756d37fcff",
   "metadata": {},
   "source": [
    "use clova : https://github.com/clovaai/CRAFT-pytorch\n",
    "\n",
    "https://github.com/fcakyon/craft-text-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e900f5a-2adc-4f01-9555-575ef9d3a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[begin] get consumer list\n",
      "Topic: raw-data-upload, Partition: 0, Offset: 0, Key: None, Value: {\"enterpriseId\": \"dusik\",\"rawDataId\": 1,\"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\"dataType\": \"OCR\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16218  100 16218    0     0  65939      0 --:--:-- --:--:-- --:--:-- 69605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': '{\"enterpriseId\": \"dusik\", \"rawDataId\": 1, \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\", \"boundingBoxInfo\": [{\"x\": [63, 233, 233, 63], \"y\": [77, 77, 151, 151]}]}'}\n",
      "Topic: raw-data-upload, Partition: 0, Offset: 1, Key: None, Value: {\"enterpriseId\": \"dusik\",\"rawDataId\": 1,\"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\"dataType\": \"OCR\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16218  100 16218    0     0   111k      0 --:--:-- --:--:-- --:--:--  117k\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from craft_text_detector import Craft\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "\n",
    "# topic, broker list\n",
    "consumer = KafkaConsumer('raw-data-upload', \n",
    "                         bootstrap_servers='13.209.250.13:9092',\n",
    "                         enable_auto_commit=True, \n",
    "                         auto_offset_reset='earliest')\n",
    "\n",
    "# consumer list를 가져온다\n",
    "print('[begin] get consumer list')\n",
    "for message in consumer:\n",
    "    print(\"Topic: {}, Partition: {}, Offset: {}, Key: {}, Value: {}\".format( message.topic, message.partition, message.offset, message.key, message.value.decode('utf-8')))\n",
    "    produce_info_array = image_preprocessing(message)\n",
    "    output_json = boundingbox_json_generator(produce_info_array)\n",
    "    produce_boundingbox(output_json)\n",
    "    \n",
    "print('[end] get consumer list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94e7200d-98ac-48b6-a022-a34b16ef377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingbox_json_generator(produce_info_array) :\n",
    "    f = open(\"./outputs/image_text_detection.txt\", 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    bounding_box_info_array = []\n",
    "    for line in lines:\n",
    "        coordinate = line.strip()\n",
    "        coordinate_array =list(map(int, coordinate.split(',')))\n",
    "        x_info = coordinate_array[0::2]\n",
    "        y_info = coordinate_array[1::2]\n",
    "\n",
    "        bounding_box_info = OrderedDict()\n",
    "        bounding_box_info[\"x\"]=x_info\n",
    "        bounding_box_info[\"y\"]=y_info\n",
    "\n",
    "        bounding_box_info_array.append(bounding_box_info)\n",
    "    f.close()\n",
    "\n",
    "    output_json = OrderedDict()\n",
    "\n",
    "    output_json['enterpriseId']= produce_info_array[0]\n",
    "    output_json['rawDataId']= produce_info_array[1]\n",
    "    output_json['imageUrl']= produce_info_array[2]\n",
    "    output_json['boundingBoxInfo']= bounding_box_info_array\n",
    "\n",
    "    output_json = json.dumps(output_json, ensure_ascii=False)\n",
    "    \n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a51539-7d64-40e7-9909-8194bb9dc496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produce_boundingbox(output_json):\n",
    "    #producer를 할당한다\n",
    "    producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['13.209.250.13:9092'],\n",
    "                             value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "    #data-preprocessing을 토픽으로 지정하여 데이터를 전송한다\n",
    "    start = time.time()\n",
    "    for i in range(1):\n",
    "        data = {'data' : output_json}\n",
    "        print(data)\n",
    "        producer.send(\"data-preprocessing\", value=data)\n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fe622ee-f5f2-4378-9063-fe75e08edf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(message): \n",
    "    \n",
    "    msg_value = message.value.decode('utf-8')\n",
    "    msg_json = json.loads(msg_value)\n",
    "    \n",
    "        \n",
    "    # {\"enterpriseId\": \"dusik\",\n",
    "    #  \"rawDataId\": 1,\n",
    "    #  \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\n",
    "    #  \"dataType\": \"OCR\"}\n",
    "    \n",
    "    # 다운받을 이미지 url\n",
    "    url = msg_json['imageUrl']\n",
    "    # curl 요청\n",
    "    os.system(\"curl \" + url + \" > labelingTarget.jpg\")\n",
    "\n",
    "    #input json을 받아서 각 변수에 담아준다\n",
    "    enterpriseId = msg_json['enterpriseId']\n",
    "    rawDataId = msg_json['rawDataId']\n",
    "    imageUrl = url\n",
    "    \n",
    "    # set image path and export folder directory\n",
    "    image = './labelingTarget.jpg' # can be filepath, PIL image or numpy array\n",
    "    output_dir = 'outputs/'\n",
    "    \n",
    "    # create a craft instance\n",
    "    craft = Craft(output_dir=output_dir, crop_type=\"poly\", cuda=False)\n",
    "    \n",
    "    # apply craft text detection and export detected regions to output directory\n",
    "    prediction_result = craft.detect_text(image)\n",
    "    \n",
    "    # unload models from ram/gpu\n",
    "    craft.unload_craftnet_model()\n",
    "    craft.unload_refinenet_model()\n",
    "    \n",
    "    # import craft functions\n",
    "    from craft_text_detector import (\n",
    "    read_image,\n",
    "    load_craftnet_model,\n",
    "    load_refinenet_model,\n",
    "    get_prediction,\n",
    "    export_detected_regions,\n",
    "    export_extra_results,\n",
    "    empty_cuda_cache\n",
    "    )\n",
    "    \n",
    "    # read image\n",
    "    image = read_image(image)\n",
    "    \n",
    "    # load models\n",
    "    refine_net = load_refinenet_model(cuda=False)\n",
    "    craft_net = load_craftnet_model(cuda=False)\n",
    "    \n",
    "    # perform prediction\n",
    "    prediction_result = get_prediction(\n",
    "    image=image,\n",
    "    craft_net=craft_net,\n",
    "    refine_net=refine_net,\n",
    "    text_threshold=0.7,\n",
    "    link_threshold=0.4,\n",
    "    low_text=0.4,\n",
    "    cuda=False,\n",
    "    long_size=1280\n",
    "    )\n",
    "    \n",
    "    # export detected text regions\n",
    "    exported_file_paths = export_detected_regions(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    output_dir=output_dir,\n",
    "    rectify=True\n",
    "    )\n",
    "    \n",
    "    # export heatmap, detection points, box visualization\n",
    "    export_extra_results(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    heatmaps=prediction_result[\"heatmaps\"],\n",
    "    output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # unload models from gpu\n",
    "    empty_cuda_cache()\n",
    "    \n",
    "    producer_info_array = [enterpriseId, rawDataId, imageUrl]\n",
    "    return producer_info_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb03057f-acaf-43f5-85c2-e1eacc1f5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "\n",
    "# admin_client = KafkaAdminClient(\n",
    "#     bootstrap_servers=\"13.209.250.13:9092\", \n",
    "#     client_id='test'\n",
    "# )\n",
    "\n",
    "# topic_list = []\n",
    "# topic_list.append(NewTopic(name=\"data-preprocessing\", num_partitions=1, replication_factor=1))\n",
    "# admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
