{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4217afd-510c-43d9-b7a7-43756d37fcff",
   "metadata": {},
   "source": [
    "use clova : https://github.com/clovaai/CRAFT-pytorch\n",
    "\n",
    "https://github.com/fcakyon/craft-text-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e900f5a-2adc-4f01-9555-575ef9d3a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[begin] Preprocessing Start\n",
      "Recived Data\n",
      "Topic: raw-data-upload, Partition: 0, Offset: 0, Key: None, Value: {\"enterpriseId\":\"enterprise-account-id\",\"rawDataId\":1,\"projectId\":1,\"fileUrl\":\"https://belloga-dev-s3-unzip-bucket.s3.ap-northeast-2.amazonaws.com/org/real-final-test-logo-testlogoziptest.zip/kakao-logo.png\",\"fileName\":\"org/real-final-test-logo-testlogoziptest.zip/kakao-logo.png\",\"dataType\":\"OCR\"}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'imageUrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecived Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Partition: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Offset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( message\u001b[38;5;241m.\u001b[39mtopic, message\u001b[38;5;241m.\u001b[39mpartition, message\u001b[38;5;241m.\u001b[39moffset, message\u001b[38;5;241m.\u001b[39mkey, message\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m---> 25\u001b[0m produce_info_array \u001b[38;5;241m=\u001b[39m \u001b[43mimage_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m output_json \u001b[38;5;241m=\u001b[39m boundingbox_json_generator(produce_info_array)\n\u001b[1;32m     27\u001b[0m produce_boundingbox(output_json)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mimage_preprocessing\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m      4\u001b[0m msg_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(msg_value)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# {\"enterpriseId\": \"dusik\",\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#  \"rawDataId\": 1,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#  \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#  \"dataType\": \"OCR\"}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 다운받을 이미지 url\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mmsg_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimageUrl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# curl 요청\u001b[39;00m\n\u001b[1;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurl \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m > labelingTarget.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'imageUrl'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from craft_text_detector import Craft\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "\n",
    "# topic, broker list\n",
    "consumer = KafkaConsumer('raw-data-upload', \n",
    "                         bootstrap_servers='13.209.250.13:9092',\n",
    "                         enable_auto_commit=True, \n",
    "                         auto_offset_reset='earliest')\n",
    "\n",
    "# consumer list를 가져온다\n",
    "print('[begin] Preprocessing Start')\n",
    "for message in consumer:\n",
    "    print(\"Recived Data\")\n",
    "    print(\"Topic: {}, Partition: {}, Offset: {}, Key: {}, Value: {}\".format( message.topic, message.partition, message.offset, message.key, message.value.decode('utf-8')))\n",
    "    produce_info_array = image_preprocessing(message)\n",
    "    output_json = boundingbox_json_generator(produce_info_array)\n",
    "    produce_boundingbox(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e7200d-98ac-48b6-a022-a34b16ef377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingbox_json_generator(produce_info_array) :\n",
    "    f = open(\"./outputs/image_text_detection.txt\", 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    bounding_box_info_array = []\n",
    "    for line in lines:\n",
    "        coordinate = line.strip()\n",
    "        coordinate_array =list(map(int, coordinate.split(',')))\n",
    "        x_info = coordinate_array[0::2]\n",
    "        y_info = coordinate_array[1::2]\n",
    "\n",
    "        bounding_box_info = OrderedDict()\n",
    "        bounding_box_info[\"x\"]=x_info\n",
    "        bounding_box_info[\"y\"]=y_info\n",
    "\n",
    "        bounding_box_info_array.append(bounding_box_info)\n",
    "    f.close()\n",
    "\n",
    "    output_json = OrderedDict()\n",
    "\n",
    "    output_json['enterpriseId']= produce_info_array[0]\n",
    "    output_json['rawDataId']= produce_info_array[1]\n",
    "    output_json['fileUrl']= produce_info_array[2]\n",
    "    output_json['projectId'] = produce_info_array[3]\n",
    "    output_json['fileName']= produce_info_array[4]\n",
    "    \n",
    "    output_json['dataType']= \"OCR\"\n",
    "    output_json['boundingBoxInfo']= bounding_box_info_array\n",
    "\n",
    "\n",
    "    output_json = json.dumps(output_json, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a51539-7d64-40e7-9909-8194bb9dc496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produce_boundingbox(output_json):\n",
    "    #producer를 할당한다\n",
    "    producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['13.209.250.13:9092'],\n",
    "                             value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "    #data-preprocessing을 토픽으로 지정하여 데이터를 전송한다\n",
    "    start = time.time()\n",
    "    for i in range(1):\n",
    "        preprocessed_data = json.loads(output_json)\n",
    "        data = preprocessed_data\n",
    "        print(\"Produced Data\")\n",
    "        print(data)\n",
    "        producer.send(\"ocr-data-preprocessing\", value=data)\n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe622ee-f5f2-4378-9063-fe75e08edf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(message): \n",
    "    \n",
    "    msg_value = message.value.decode('utf-8')\n",
    "    msg_json = json.loads(msg_value)\n",
    "    \n",
    "        \n",
    "    # 다운받을 이미지 url\n",
    "    url = msg_json['fileUrl']\n",
    "    # curl 요청\n",
    "    os.system(\"curl \" + url + \" > labelingTarget.jpg\")\n",
    "\n",
    "    #input json을 받아서 각 변수에 담아준다\n",
    "    enterpriseId = msg_json['enterpriseId']\n",
    "    rawDataId = msg_json['rawDataId']\n",
    "    projectId = msg_json['projectId']\n",
    "    fileName = msg_json['fileName']\n",
    "    imageUrl = url\n",
    "    \n",
    "    # set image path and export folder directory\n",
    "    image = './labelingTarget.jpg' # can be filepath, PIL image or numpy array\n",
    "    output_dir = 'outputs/'\n",
    "    \n",
    "    # create a craft instance\n",
    "    craft = Craft(output_dir=output_dir, crop_type=\"poly\", cuda=False)\n",
    "    \n",
    "    # apply craft text detection and export detected regions to output directory\n",
    "    prediction_result = craft.detect_text(image)\n",
    "    \n",
    "    # unload models from ram/gpu\n",
    "    craft.unload_craftnet_model()\n",
    "    craft.unload_refinenet_model()\n",
    "    \n",
    "    # import craft functions\n",
    "    from craft_text_detector import (\n",
    "    read_image,\n",
    "    load_craftnet_model,\n",
    "    load_refinenet_model,\n",
    "    get_prediction,\n",
    "    export_detected_regions,\n",
    "    export_extra_results,\n",
    "    empty_cuda_cache\n",
    "    )\n",
    "    \n",
    "    # read image\n",
    "    image = read_image(image)\n",
    "    \n",
    "    # load models\n",
    "    refine_net = load_refinenet_model(cuda=False)\n",
    "    craft_net = load_craftnet_model(cuda=False)\n",
    "    \n",
    "    # perform prediction\n",
    "    prediction_result = get_prediction(\n",
    "    image=image,\n",
    "    craft_net=craft_net,\n",
    "    refine_net=refine_net,\n",
    "    text_threshold=0.7,\n",
    "    link_threshold=0.4,\n",
    "    low_text=0.4,\n",
    "    cuda=False,\n",
    "    long_size=1280\n",
    "    )\n",
    "    \n",
    "    # export detected text regions\n",
    "    exported_file_paths = export_detected_regions(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    output_dir=output_dir,\n",
    "    rectify=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # export heatmap, detection points, box visualization\n",
    "    export_extra_results(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    heatmaps=prediction_result[\"heatmaps\"],\n",
    "    output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # unload models from gpu\n",
    "    empty_cuda_cache()\n",
    "    \n",
    "    producer_info_array = [enterpriseId, rawDataId, imageUrl, projectId, fileName]\n",
    "    return producer_info_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb03057f-acaf-43f5-85c2-e1eacc1f5c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TopicAlreadyExistsError",
     "evalue": "[Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='ocr-data-preprocessing', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='ocr-data-preprocessing', error_code=36, error_message=\"Topic 'ocr-data-preprocessing' already exists.\")])'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m topic_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m topic_list\u001b[38;5;241m.\u001b[39mappend(NewTopic(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mocr-data-preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_partitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, replication_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43madmin_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/admin/client.py:461\u001b[0m, in \u001b[0;36mKafkaAdminClient.create_topics\u001b[0;34m(self, new_topics, timeout_ms, validate_only)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for CreateTopics v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has not yet been added to KafkaAdminClient.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(version))\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# TODO convert structs to a more pythonic interface\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# TODO raise exceptions if errors\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request_to_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/admin/client.py:407\u001b[0m, in \u001b[0;36mKafkaAdminClient._send_request_to_controller\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoError:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error_type(\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m failed with response \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(request, response))\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m: [Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='ocr-data-preprocessing', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='ocr-data-preprocessing', error_code=36, error_message=\"Topic 'ocr-data-preprocessing' already exists.\")])'."
     ]
    }
   ],
   "source": [
    "# from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "\n",
    "# admin_client = KafkaAdminClient(\n",
    "#     bootstrap_servers=\"13.209.250.13:9092\", \n",
    "#     client_id='test'\n",
    "# )\n",
    "\n",
    "# topic_list = []\n",
    "# topic_list.append(NewTopic(name=\"ocr-data-preprocessing\", num_partitions=1, replication_factor=1))\n",
    "# admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca6dc2-592c-4848-8991-086ba132bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from craft_text_detector import Craft\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "\n",
    "# topic, broker list\n",
    "consumer = KafkaConsumer('raw-data-upload', \n",
    "                         bootstrap_servers='13.209.250.13:9092',\n",
    "                         enable_auto_commit=True, \n",
    "                         auto_offset_reset='earliest')\n",
    "\n",
    "# consumer list를 가져온다\n",
    "print('[begin] Preprocessing Start')\n",
    "for message in consumer:\n",
    "    print(\"Recived Data\")\n",
    "    print(\"Topic: {}, Partition: {}, Offset: {}, Key: {}, Value: {}\".format( message.topic, message.partition, message.offset, message.key, message.value.decode('utf-8')))\n",
    "    produce_info_array = image_preprocessing(message)\n",
    "    output_json = boundingbox_json_generator(produce_info_array)\n",
    "    produce_boundingbox(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e976bb-7d47-4f65-94d1-e9989a5c38dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
