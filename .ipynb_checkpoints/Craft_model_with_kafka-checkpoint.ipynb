{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4217afd-510c-43d9-b7a7-43756d37fcff",
   "metadata": {},
   "source": [
    "use clova : https://github.com/clovaai/CRAFT-pytorch\n",
    "\n",
    "https://github.com/fcakyon/craft-text-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e900f5a-2adc-4f01-9555-575ef9d3a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[begin] get consumer list\n",
      "Topic: raw-data-upload, Partition: 0, Offset: 0, Key: None, Value: {\"enterpriseId\": \"dusik\",\"rawDataId\": 1,\"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\"dataType\": \"OCR\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16218  100 16218    0     0   129k      0 --:--:-- --:--:-- --:--:--  143k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': '{\"enterpriseId\": \"dusik\", \"rawDataId\": 1, \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\", \"boundingBoxInfo\": [{\"x\": [63, 233, 233, 63], \"y\": [77, 77, 151, 151]}]}'}\n",
      "Topic: raw-data-upload, Partition: 0, Offset: 1, Key: None, Value: {\"enterpriseId\": \"dusik\",\"rawDataId\": 1,\"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\"dataType\": \"OCR\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16218  100 16218    0     0   155k      0 --:--:-- --:--:-- --:--:--  168k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': '{\"enterpriseId\": \"dusik\", \"rawDataId\": 1, \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\", \"boundingBoxInfo\": [{\"x\": [63, 233, 233, 63], \"y\": [77, 77, 151, 151]}]}'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# consumer list를 가져온다\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[begin] get consumer list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Partition: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Offset: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( message\u001b[38;5;241m.\u001b[39mtopic, message\u001b[38;5;241m.\u001b[39mpartition, message\u001b[38;5;241m.\u001b[39moffset, message\u001b[38;5;241m.\u001b[39mkey, message\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     24\u001b[0m     produce_info_array \u001b[38;5;241m=\u001b[39m image_preprocessing(message)\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/.python-3.8/lib/python3.8/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/selectors.py:558\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 558\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from craft_text_detector import Craft\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "\n",
    "# topic, broker list\n",
    "consumer = KafkaConsumer('raw-data-upload', \n",
    "                         bootstrap_servers='13.209.250.13:9092',\n",
    "                         enable_auto_commit=True, \n",
    "                         auto_offset_reset='earliest')\n",
    "\n",
    "# consumer list를 가져온다\n",
    "print('[begin] get consumer list')\n",
    "for message in consumer:\n",
    "    print(\"Topic: {}, Partition: {}, Offset: {}, Key: {}, Value: {}\".format( message.topic, message.partition, message.offset, message.key, message.value.decode('utf-8')))\n",
    "    produce_info_array = image_preprocessing(message)\n",
    "    output_json = boundingbox_json_generator(produce_info_array)\n",
    "    produce_boundingbox(output_json)\n",
    "    \n",
    "print('[end] get consumer list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e7200d-98ac-48b6-a022-a34b16ef377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingbox_json_generator(produce_info_array) :\n",
    "    f = open(\"./outputs/image_text_detection.txt\", 'r')\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    bounding_box_info_array = []\n",
    "    for line in lines:\n",
    "        coordinate = line.strip()\n",
    "        coordinate_array =list(map(int, coordinate.split(',')))\n",
    "        x_info = coordinate_array[0::2]\n",
    "        y_info = coordinate_array[1::2]\n",
    "\n",
    "        bounding_box_info = OrderedDict()\n",
    "        bounding_box_info[\"x\"]=x_info\n",
    "        bounding_box_info[\"y\"]=y_info\n",
    "\n",
    "        bounding_box_info_array.append(bounding_box_info)\n",
    "    f.close()\n",
    "\n",
    "    output_json = OrderedDict()\n",
    "\n",
    "    output_json['enterpriseId']= produce_info_array[0]\n",
    "    output_json['rawDataId']= produce_info_array[1]\n",
    "    output_json['imageUrl']= produce_info_array[2]\n",
    "    output_json['boundingBoxInfo']= bounding_box_info_array\n",
    "\n",
    "    output_json = json.dumps(json.JSONDecoder().decode(output_json),ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34a51539-7d64-40e7-9909-8194bb9dc496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produce_boundingbox(output_json):\n",
    "    #producer를 할당한다\n",
    "    producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['13.209.250.13:9092'],\n",
    "                             value_serializer=lambda x: dumps(x).encode('utf-8'))\n",
    "\n",
    "    #data-preprocessing을 토픽으로 지정하여 데이터를 전송한다\n",
    "    start = time.time()\n",
    "    for i in range(1):\n",
    "        data = {'data' : output_json}\n",
    "        print(data)\n",
    "        producer.send(\"ocr-data-preprocessing\", value=data)\n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fe622ee-f5f2-4378-9063-fe75e08edf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(message): \n",
    "    \n",
    "    msg_value = message.value.decode('utf-8')\n",
    "    msg_json = json.loads(msg_value)\n",
    "    \n",
    "        \n",
    "    # {\"enterpriseId\": \"dusik\",\n",
    "    #  \"rawDataId\": 1,\n",
    "    #  \"imageUrl\": \"https://belloga-dev-s3-raw-data-bucket.s3.ap-northeast-2.amazonaws.com/dev-data/00000001+-+%EB%B3%B5%EC%82%AC%EB%B3%B8.png\",\n",
    "    #  \"dataType\": \"OCR\"}\n",
    "    \n",
    "    # 다운받을 이미지 url\n",
    "    url = msg_json['imageUrl']\n",
    "    # curl 요청\n",
    "    os.system(\"curl \" + url + \" > labelingTarget.jpg\")\n",
    "\n",
    "    #input json을 받아서 각 변수에 담아준다\n",
    "    enterpriseId = msg_json['enterpriseId']\n",
    "    rawDataId = msg_json['rawDataId']\n",
    "    imageUrl = url\n",
    "    \n",
    "    # set image path and export folder directory\n",
    "    image = './labelingTarget.jpg' # can be filepath, PIL image or numpy array\n",
    "    output_dir = 'outputs/'\n",
    "    \n",
    "    # create a craft instance\n",
    "    craft = Craft(output_dir=output_dir, crop_type=\"poly\", cuda=False)\n",
    "    \n",
    "    # apply craft text detection and export detected regions to output directory\n",
    "    prediction_result = craft.detect_text(image)\n",
    "    \n",
    "    # unload models from ram/gpu\n",
    "    craft.unload_craftnet_model()\n",
    "    craft.unload_refinenet_model()\n",
    "    \n",
    "    # import craft functions\n",
    "    from craft_text_detector import (\n",
    "    read_image,\n",
    "    load_craftnet_model,\n",
    "    load_refinenet_model,\n",
    "    get_prediction,\n",
    "    export_detected_regions,\n",
    "    export_extra_results,\n",
    "    empty_cuda_cache\n",
    "    )\n",
    "    \n",
    "    # read image\n",
    "    image = read_image(image)\n",
    "    \n",
    "    # load models\n",
    "    refine_net = load_refinenet_model(cuda=False)\n",
    "    craft_net = load_craftnet_model(cuda=False)\n",
    "    \n",
    "    # perform prediction\n",
    "    prediction_result = get_prediction(\n",
    "    image=image,\n",
    "    craft_net=craft_net,\n",
    "    refine_net=refine_net,\n",
    "    text_threshold=0.7,\n",
    "    link_threshold=0.4,\n",
    "    low_text=0.4,\n",
    "    cuda=False,\n",
    "    long_size=1280\n",
    "    )\n",
    "    \n",
    "    # export detected text regions\n",
    "    exported_file_paths = export_detected_regions(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    output_dir=output_dir,\n",
    "    rectify=True\n",
    "    )\n",
    "    \n",
    "    # export heatmap, detection points, box visualization\n",
    "    export_extra_results(\n",
    "    image=image,\n",
    "    regions=prediction_result[\"boxes\"],\n",
    "    heatmaps=prediction_result[\"heatmaps\"],\n",
    "    output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # unload models from gpu\n",
    "    empty_cuda_cache()\n",
    "    \n",
    "    producer_info_array = [enterpriseId, rawDataId, imageUrl]\n",
    "    return producer_info_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb03057f-acaf-43f5-85c2-e1eacc1f5c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='ocr-data-preprocessing', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"13.209.250.13:9092\", \n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_list = []\n",
    "topic_list.append(NewTopic(name=\"ocr-data-preprocessing\", num_partitions=1, replication_factor=1))\n",
    "admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca6dc2-592c-4848-8991-086ba132bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
